# ğŸ§ª Experiment Report: Attention Is All You Need

> **Paper:** Vaswani et al., 2017 â€” *Attention Is All You Need*  
> **Organization:** Google Brain  
> **Stage:** Representation  

---

## âš™ï¸ Experiment Setup
| Component | Details |
|------------|----------|
| Model | Transformer (Encoderâ€“Decoder), TBD |
| Dataset | TBD |
| Training | TBD |
| Hardware | TBD |
| Framework | TBD |

---

## ğŸ“Š Quantitative Results
| Metric | Original (Paper) | Reproduced | Deviation | Notes |
|---------|------------------|-------------|------------|--------|
| BLEU (ENâ†’DE) | 28.4 | TBD | TBD | TBD |
| Perplexity | 4.6 | TBD | TBD | TBD |
| Training Speed | 3.5Ã— RNN baseline | TBD | TBD | TBD |
| Params | 65M | TBD | TBD | TBD |
| Training Time | 12h (8 GPUs) | TBD | TBD | TBD |

---

## ğŸ“ˆ Learning Curves
*(insert plots or text summary after experiments)*  
- TBD

---

## ğŸ” Attention Visualization
*(add attention heatmaps or screenshots)*  
- TBD

---

## ğŸ’¬ Observations & Analysis
1. TBD  
2. TBD  
3. TBD  

---

## ğŸ§  Lessons Learned
- Scale up is the key to improve the model efficient training on large model. Multi head approach enables this. 
-  

---

## ğŸ“š References

```bibtex
@article{vaswani2017attention,
  title     = {Attention Is All You Need},
  author    = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Åukasz Kaiser and Illia Polosukhin},
  journal   = {arXiv preprint arXiv:1706.03762},
  year      = {2017},
  url       = {https://arxiv.org/abs/1706.03762}
}

@article{an2025reproduce_attention,
  title     = {Reproduction Report: Attention Is All You Need},
  author    = {Duo An},
  journal   = {ReproduceAI Project},
  year      = {2025},
  url       = {https://github.com/duoan/ReproduceAI/stage2_representation/2017_AttentionIsAllYouNeed}
}
```
