# üìò Paper Replication: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale

> **Authors:** Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
> Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby
> **Published:** 2020  
> **Organization:** Google  
> **Stage:** Representation

![ViT](./figures/vit.png)

---

## üéØ Replication Objectives

- Implement the model from the paper in a modular, transparent way.
- Replicate key experiments and results.
- Verify main claims and discuss deviations.
- Document issues, reproducibility challenges, and results.

---

## üß© Core Ideas

1. Using `Transformer Architecture` to solve computer vision tasks.
2. `PatchEmbedding` split the 224x224 images into patches and flatten into a sequence
3. Pre-train using treat it as classic image classification problem,
    1. add `[CLS]` token, add learnable parameter
4. add a shared learnable parameter for position, shape with [1, 1 + num_patches, hidden_size].
    1. Where the patches near each other in the grid should have correlated positions
    2. Patches far apart should encode distant spatial relations
    3. Not like text, ViT uses learnable position embeddings, because images have bounded size, learned embeddings adapt
       better to 2D spatial structure
4. Only use the encoder + classifier head

---

## ‚öôÔ∏è Implementation Plan

| Component  | Description |
|------------|-------------|
| Model      | Done        |
| Dataset    | Imagenette  |
| Evaluation | Acc         |
| Notes      | WIP         |

---

## Model Visualization
```text
VisionTransformer(
  (embedding): ViTEmbedding(
    (patch_embeddings): ViTPatchEmbedding(
      (projector): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
      (flatten): Flatten(start_dim=2, end_dim=-1)
    )
    (dropout): Dropout(p=0.0, inplace=False)
  )
  (encoder): ViTEncoder(
    (layers): ModuleList(
      (0-11): 12 x Sequential(
        (0): ViTAttention(
          (attn_drop): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (query): Linear(in_features=768, out_features=768, bias=False)
          (key): Linear(in_features=768, out_features=768, bias=False)
          (value): Linear(in_features=768, out_features=768, bias=False)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_drop): Dropout(p=0.0, inplace=False)
        )
        (1): ViTFeedForward(
          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (net): Sequential(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
            (4): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (mlp_head): Linear(in_features=768, out_features=10, bias=True)
)
```

![model](./figures/model.png)

## Model Parameters

```text
================================================================
Total params: 85,606,666
Trainable params: 85,606,666
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 18.38
Forward/backward pass size (MB): 11336.94
Params size (MB): 326.56
Estimated Total Size (MB): 11681.88
----------------------------------------------------------------
```

## üß™ Expected Results

| Metric | Target | Notes |
|--------|--------|-------|
| TBD    | TBD    | TBD   |

---

## üß≠ Notes

- <Write short comments about the reproduction context>

---

üìÖ Generated by **ReplicateAI**
