# üß™ Experiment Report: Vision Transformer (ViT) on Imagenette

> An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (2020)

![ViT](./figures/vit.png)

---

## üéØ Objective

- Re-implement a compact Vision Transformer and evaluate on Imagenette (10-class subset of ImageNet) to validate core ideas of ViT: patch embeddings, learnable position encodings, encoder-only transformer, and a `[CLS]`-pooled classifier head.

---

## üß± Setup

- Dataset: Imagenette (image size 224√ó224)
- Model: Encoder-only ViT with learnable patch embeddings and position encodings; classifier head on `[CLS]`
- Loss/Optimizer: CrossEntropyLoss (with/without label smoothing), AdamW
- Augmentations: RandomResizedCrop, HorizontalFlip, ColorJitter; standard ImageNet normalization
- Hardware: Single-machine training

---

## üß† Model Snapshot

The implementation mirrors the paper‚Äôs encoder stack: multi-head self-attention + MLP blocks with LayerNorm and residuals. Patch size is 16√ó16 with a Conv2d projector, plus a learnable `[CLS]` token and positional embeddings.

![Model](./figures/model.png)

---

## ‚öôÔ∏è Training Configuration

- Example configuration that achieved the best validation accuracy in this replication:
  - hidden_size: 256
  - num_hidden_layers: 4
  - num_attention_heads: 4
  - intermediate_size: 1024
  - hidden_dropout_prob: 0.1
  - layer_norm_eps: 1e-6
  - label_smoothing: 0.1
  - learning_rate: 1e-5 to 3e-5
  - batch_size: 32
  - epochs: up to 100 (early-stopped around 35)

Data augmentation used:

```python
from torchvision import transforms

train_tfm = transforms.Compose([
    transforms.RandomResizedCrop(224, scale=(0.6, 1.0)),
    transforms.RandomHorizontalFlip(),
    transforms.ColorJitter(0.4, 0.4, 0.4, 0.1),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
test_tfm = transforms.Compose([
    transforms.Resize(256), transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
```

---

## üìä Results

| hidden_size | layers | heads | interm | drop | eps  | label_smooth | lr   | bs | epochs (stop) | Val Acc |
|-------------|--------|-------|--------|------|------|---------------|------|----|---------------|---------|
| 256         | 4      | 4     | 1024   | 0.1  | 1e-6 | 0.0           | 3e-5 | 32 | 49 (30)       | 0.368   |
| 256         | 4      | 4     | 1024   | 0.1  | 1e-6 | 0.1           | 3e-5 | 32 | 100 (35)      | 0.644   |
| 256         | 4      | 4     | 1024   | 0.1  | 1e-6 | 0.1           | 1e-5 | 32 | 100 (35)      | 0.644   |

Notes:
- Label smoothing at Œµ = 0.1 consistently improved stability and accuracy.
- Strong data augmentation narrowed the gap to paper-scale results on this small dataset.

---

## üìà Observations

- With Œµ = 0.1, early optimization became notably smoother; loss oscillations were reduced.
- Attention maps appeared less collapsed onto a few patches when label smoothing was used, indicating healthier patch-level representations.
- Training remained stable without dropout in attention/MLP, but 0.1 dropout yielded slightly better generalization.

---

## üí¨ Analysis

- Why label smoothing helps ViT:
  - It caps early logit spikes from the `[CLS]` token, which otherwise can push over-confident predictions.
  - Encourages more distributed attention across patches, improving representation quality.
  - Acts as implicit regularization, reducing overfitting on small datasets like Imagenette.

- Sensitivities:
  - Learning rate: 3e-5 performed best here; 1e-5 was similar after early stop, but slower to reach peak.
  - Depth/width: On Imagenette, small ViT (4√ó4 heads, 256 dim) is sufficient; larger models risk overfitting.

---

## üß© Conclusion

- A compact ViT reproduces the paper‚Äôs core behavior on a small-scale benchmark. Label smoothing and moderate augmentation are key to stable, competitive results.
- Further gains likely come from stronger regularization (Mixup/CutMix), longer training, and larger datasets/pretraining.

---

## üîÅ Reproducibility

- Code: see `src/model.py`, `src/main.py` and the notebook in `notebook/Vision Transformer_demo.ipynb`.
- Data: `notebook/data/imagenette/imagenette2` (Imagenette v2); follow README for setup.
- Seed and environment: fix random seeds and use deterministic ops where possible; single-GPU runs were used here.

---

## üìö References

- Dosovitskiy et al., 2020. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.

üìÖ Generated by **ReplicateAI**
