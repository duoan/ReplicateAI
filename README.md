# ğŸ§  ReproduceAI

> **Recreating every milestone in Machine Learning and Artificial Intelligence â€” from Transformers to Perceptrons.**

[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Contributions welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg)]()
[![Status](https://img.shields.io/badge/Project-Active-blue.svg)]()

---

## ğŸš€ Overview

**ReproduceAI** is an open initiative to **rebuild and verify every major paper in ML/AI history**,  
starting from modern **foundation models (2023â€“2025)** and tracing backward to the origins of AI.

We believe that **understanding AI means rebuilding it â€” line by line, layer by layer.**

---

## ğŸ§© Project Vision

> â€œBecause science means reproducibility.â€

- ğŸ“œ **Goal**: Faithfully re-implement influential ML/AI papers with open code, datasets, and experiments  
- ğŸ§± **Scope**: From *Qwen2.5 (2025)* to *Perceptron (1958)*  
- ğŸ§  **Approach**: Reverse timeline â€” start with Foundation Models, then trace history backward  
- ğŸ§¾ **Output**: Each paper becomes a self-contained, reproducible module with reports and experiments  

---

## ğŸª Stage 1 â€” Foundation & Multimodal Era (2023â€“2025)

> *The golden age of open-source foundation models.*

| Year | Paper / Model | Organization | Why It Matters | Reproduce Goal | Status |
|------|----------------|--------------|----------------|----------------|---------|
| **2025** | **Qwen2.5** | Alibaba | Fully open multimodal model (text + image) | Rebuild text/image pipeline | ğŸ§­ Planned |
| **2025** | **DeepSeek-V2** | DeepSeek | MoE + RLHF efficiency breakthrough | Reproduce expert routing and reward pipeline | ğŸ§­ Planned |
| **2025** | **Claude 3 Family** | Anthropic | Leading alignment via Constitutional AI | Explore rule-based alignment principles | ğŸ§­ Planned |
| **2024** | **LLaMA 3** | Meta | Open foundation model standard | Implement scaled transformer + tokenizer | ğŸ§­ Planned |
| **2024** | **Mixtral 8Ã—7B** | Mistral | Sparse Mixture-of-Experts architecture | Implement routing + expert parallelism | ğŸ§­ Planned |
| **2024** | **Phi-2 / Phi-3** | Microsoft | Small but high-quality model; data-centric | Rebuild synthetic data pipeline | ğŸ§­ Planned |
| **2024** | **Gemini 1 / 1.5** | Google DeepMind | Vision + Text + Reasoning | Prototype multimodal reasoning pipeline | ğŸ§­ Planned |
| **2023** | **Qwen-VL** | Alibaba | Vision-language alignment model | Reproduce visual encoder + text fusion | ğŸ§­ Planned |
| **2023** | **BLIP-2 / MiniGPT-4** | Salesforce / HKU | Lightweight multimodal bridging | Implement pretrain connector | ğŸ§­ Planned |
| **2023** | **LLaMA 1 / 2** | Meta | Open LLM baseline | Implement tokenizer + attention stack | ğŸ§­ Planned |

---

## ğŸ” Stage 2 â€” Representation & Sequence Models (2013â€“2020)

| Year | Paper | Author | Goal | Status |
|------|--------|---------|--------|---------|
| 2018 | BERT | Devlin et al. | Masked Language Modeling | ğŸ§­ Planned |
| 2017 | Transformer | Vaswani et al. | â€œAttention Is All You Needâ€ | ğŸ§­ Planned |
| 2014 | Seq2Seq | Sutskever et al. | Encoder-decoder translation | ğŸ§­ Planned |
| 2013 | Word2Vec | Mikolov et al. | Learn word embeddings | ğŸ§­ Planned |
| 2015 | Bahdanau Attention | Bahdanau et al. | RNN + Attention | ğŸ§­ Planned |

---

## ğŸ§© Stage 3 â€” Deep Learning Renaissance (2006â€“2014)

| Year | Paper | Author | Goal | Status |
|------|--------|---------|--------|---------|
| 2015 | ResNet | He et al. | Residual learning | ğŸ§­ Planned |
| 2014 | VGG | Simonyan et al. | Deep CNN architectures | ğŸ§­ Planned |
| 2012 | AlexNet | Krizhevsky et al. | GPU-based CNN | ğŸ§­ Planned |
| 2006 | DBN / RBM | Hinton | Layer-wise pretraining | ğŸ§­ Planned |

---

## ğŸ“Š Stage 4 â€” Statistical Learning Era (1990sâ€“2000s)

| Year | Paper | Author | Goal | Status |
|------|--------|---------|--------|---------|
| 2001 | Random Forests | Breiman | Ensemble learning | ğŸ§­ Planned |
| 1997 | AdaBoost | Freund & Schapire | Boosting algorithms | ğŸ§­ Planned |
| 1995 | SVM | Vapnik | Maximum margin classifier | ğŸ§­ Planned |
| 1977 | EM Algorithm | Dempster et al. | Expectation-Maximization | ğŸ§­ Planned |

---

## ğŸ§¬ Stage 5 â€” Early Neural Foundations (1950sâ€“1980s)

| Year | Paper | Author | Goal | Status |
|------|--------|---------|--------|---------|
| 1986 | Backpropagation | Rumelhart et al. | Gradient-based learning | ğŸ§­ Planned |
| 1985 | Boltzmann Machine | Hinton et al. | Generative stochastic model | ğŸ§­ Planned |
| 1982 | Hopfield Network | Hopfield | Associative memory | ğŸ§­ Planned |
| 1958 | Perceptron | Rosenblatt | Linear separability | ğŸ§­ Planned |

---

## ğŸ“ Repository Structure

```

ReproduceAI/
â”œâ”€â”€ stage1_foundation/
â”‚   â”œâ”€â”€ 2025_Qwen2.5/
â”‚   â”œâ”€â”€ 2024_LLaMA3/
â”‚   â””â”€â”€ 2023_CLIP/
â”œâ”€â”€ stage2_representation/
â”‚   â”œâ”€â”€ 2018_BERT/
â”‚   â”œâ”€â”€ 2017_Transformer/
â”‚   â””â”€â”€ 2013_Word2Vec/
â”œâ”€â”€ stage3_deep_renaissance/
â”‚   â”œâ”€â”€ 2015_ResNet/
â”‚   â”œâ”€â”€ 2012_AlexNet/
â”‚   â””â”€â”€ 2006_DBN/
â”œâ”€â”€ stage4_statistical/
â”‚   â”œâ”€â”€ 2001_RandomForest/
â”‚   â””â”€â”€ 1995_SVM/
â””â”€â”€ stage5_foundations/
â”œâ”€â”€ 1986_Backprop/
â””â”€â”€ 1958_Perceptron/

```

Each paper module includes:
```

ğŸ“„ README.md   â€” Paper summary & objective
ğŸ“˜ report.md   â€” Reproduction results & analysis
ğŸ““ notebook/   â€” Interactive demo
ğŸ’» src/        â€” Core implementation
ğŸ”— references.bib â€” Original citation

````

---

## ğŸ¤ Contributing

We welcome contributions from researchers, engineers, and students who believe in reproducibility.

1. Fork the repo  
2. Pick a paper or model not yet implemented  
3. Follow the [Paper Template](paper_template/README.md)  
4. Submit a PR with your code and report  

âœ… **Please include**:
- clear code (PyTorch / JAX / NumPy)
- short experiment or visualization
- reproducibility notes or deviations

---

## ğŸ§® Progress Overview

| Stage | Era | Progress |
|--------|-----|-----------|
| ğŸª Foundation (2023â€“2025) | Modern LLM & Multimodal | â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 0% |
| ğŸ” Representation (2013â€“2020) | Transformers & Embeddings | â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 0% |
| ğŸ§© Deep Renaissance (2006â€“2014) | CNN Era | â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 0% |
| ğŸ“Š Statistical (1990sâ€“2000s) | Classical ML | â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 0% |
| ğŸ§¬ Foundations (1950sâ€“1980s) | Neural Origins | â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 0% |

---

## ğŸ“š Citation

If you use or reference this project, please cite:

```bibtex
@misc{reproduceai2025,
  author = {ReproduceAI Contributors},
  title = {ReproduceAI: Rebuilding the History of Machine Learning and Artificial Intelligence},
  year = {2025},
  url = {https://github.com/<yourname>/ReproduceAI}
}
```

---

## ğŸ’¬ Motto

> â€œReproduce. Verify. Understand.â€

---

â­ï¸ **Star this repo if you believe reproducibility is the foundation of true intelligence.**


